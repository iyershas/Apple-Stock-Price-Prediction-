{"cells":[{"cell_type":"code","source":["#importing necessary libraries\nfrom pyspark.sql import SQLContext, Window\nfrom pyspark.sql.functions import *\nimport pyspark.sql.functions as f\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.functions import abs, sqrt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#reading in the data\n#creating an SQL Context\nsqlContext = SQLContext(sc)\n##Creating a dataframe by querying data from the csv file loaded in the Cluster\n##Querying Apple Close Price data from 2016-2017 by applying the year function on the date column and specifying it to be between 2016 & 2017\ndf = sqlContext.sql(\"SELECT date, close FROM aapl__2__1028f_csv WHERE YEAR(date) BETWEEN 2016 AND 2017\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["##displaying the dataframe - two columns, data and close price from 2016-2017\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----------+\n               date|     close|\n+-------------------+----------+\n2016-01-04 00:00:00|105.349998|\n2016-01-05 00:00:00|102.709999|\n2016-01-06 00:00:00|100.699997|\n2016-01-07 00:00:00| 96.449997|\n2016-01-08 00:00:00| 96.959999|\n2016-01-11 00:00:00| 98.529999|\n2016-01-12 00:00:00| 99.959999|\n2016-01-13 00:00:00| 97.389999|\n2016-01-14 00:00:00| 99.519997|\n2016-01-15 00:00:00| 97.129997|\n2016-01-19 00:00:00| 96.660004|\n2016-01-20 00:00:00| 96.790001|\n2016-01-21 00:00:00| 96.300003|\n2016-01-22 00:00:00|101.419998|\n2016-01-25 00:00:00| 99.440002|\n2016-01-26 00:00:00| 99.989998|\n2016-01-27 00:00:00| 93.419998|\n2016-01-28 00:00:00| 94.089996|\n2016-01-29 00:00:00| 97.339996|\n2016-02-01 00:00:00|     96.43|\n+-------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["##QUESTION 1\n###Using the window function to sort the date column in ascending order using the Order by function\npartitionwindow = Window.orderBy(\"date\")\n## Creating time limited features - lag1 & lag2, where lag1 is the close price pushed by one day using the record difference property and lag2 by 2 days\nfeature1 =lag(\"close\",1).over(partitionwindow)\nfeature2 =lag(\"close\",2).over(partitionwindow)\n##Generating the columns lag1 and lag2 within our dataframe using the withColumn function \ndf = df.withColumn(\"lag1\", feature1)\ndf = df.withColumn(\"lag2\", feature2)\ndf = df.na.drop()\n##Displaying the dataframe\ndf.show(20)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----------+----------+----------+\n               date|     close|      lag1|      lag2|\n+-------------------+----------+----------+----------+\n2016-01-06 00:00:00|100.699997|102.709999|105.349998|\n2016-01-07 00:00:00| 96.449997|100.699997|102.709999|\n2016-01-08 00:00:00| 96.959999| 96.449997|100.699997|\n2016-01-11 00:00:00| 98.529999| 96.959999| 96.449997|\n2016-01-12 00:00:00| 99.959999| 98.529999| 96.959999|\n2016-01-13 00:00:00| 97.389999| 99.959999| 98.529999|\n2016-01-14 00:00:00| 99.519997| 97.389999| 99.959999|\n2016-01-15 00:00:00| 97.129997| 99.519997| 97.389999|\n2016-01-19 00:00:00| 96.660004| 97.129997| 99.519997|\n2016-01-20 00:00:00| 96.790001| 96.660004| 97.129997|\n2016-01-21 00:00:00| 96.300003| 96.790001| 96.660004|\n2016-01-22 00:00:00|101.419998| 96.300003| 96.790001|\n2016-01-25 00:00:00| 99.440002|101.419998| 96.300003|\n2016-01-26 00:00:00| 99.989998| 99.440002|101.419998|\n2016-01-27 00:00:00| 93.419998| 99.989998| 99.440002|\n2016-01-28 00:00:00| 94.089996| 93.419998| 99.989998|\n2016-01-29 00:00:00| 97.339996| 94.089996| 93.419998|\n2016-02-01 00:00:00|     96.43| 97.339996| 94.089996|\n2016-02-02 00:00:00| 94.480003|     96.43| 97.339996|\n2016-02-03 00:00:00| 96.349998| 94.480003|     96.43|\n+-------------------+----------+----------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["##PREPARING THE DATA TO PERFORM MACHINE LEARNING\n##Selecting the relavent columns - where lag1 & lag2 are the features and the close price is the label/observed value we are trying to predict\ndf = df.select('lag1','lag2','close')\n##Using the VectorAssembler function to create an array of vectors containing the values in the lag1 and lag2 columns - FEATURES \nvectorAssembler = VectorAssembler(inputCols = ['lag1','lag2'], outputCol = 'features')\n##Transforming the Dataframe containing the lag1, lag2 and close price columns by applying the Vector Assembler function on the input columns\ndf1 = vectorAssembler.transform(df)\n##Selecting the relavent columns from our transformed dataframe and st\ndf2 = df1.select('features','close')\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------+\n            features|     close|\n+--------------------+----------+\n[102.709999,105.3...|100.699997|\n[100.699997,102.7...| 96.449997|\n[96.449997,100.69...| 96.959999|\n[96.959999,96.449...| 98.529999|\n[98.529999,96.959...| 99.959999|\n[99.959999,98.529...| 97.389999|\n[97.389999,99.959...| 99.519997|\n[99.519997,97.389...| 97.129997|\n[97.129997,99.519...| 96.660004|\n[96.660004,97.129...| 96.790001|\n[96.790001,96.660...| 96.300003|\n[96.300003,96.790...|101.419998|\n[101.419998,96.30...| 99.440002|\n[99.440002,101.41...| 99.989998|\n[99.989998,99.440...| 93.419998|\n[93.419998,99.989...| 94.089996|\n[94.089996,93.419...| 97.339996|\n[97.339996,94.089...|     96.43|\n   [96.43,97.339996]| 94.480003|\n   [94.480003,96.43]| 96.349998|\n+--------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["###QUESTION 2\n##Doing a random Train Test split, where 70% of the dataframe is randomly selected for training and 30% of the data is randomly selected for testing\nsplits = df2.randomSplit([0.7, 0.3])\n##Storing the 70% data for training in train_df\ntrain_df = splits[0]\n##Storing the 30% data for testing in test_df\ntest_df = splits[1]\n##Initiating the LinearRegression model from the SparkMLLib and specifying the features and the observed value/labels\nlr = LinearRegression(featuresCol = 'features', labelCol='close')\n##Fitting the Linear Regression Model on the training data and storing the model in lr_model\nlr_model = lr.fit(train_df)\n##Printing the coefficients of the slope of our Linear Regression Model and the Intercept\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))\n##Summarizing our model over the training set and printing out two important metrics - R2 and the RMSE\ntrainingSummary = lr_model.summary\n##RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model's predicted values.\nprint(\"Root Mean Squared Error (RMSE) on train data = %g\" % trainingSummary.rootMeanSquaredError)\n##R2 is \"(total variance explained by model) / total variance.” So if it is 100%, the two variables are perfectly correlated, i.e., with no variance at all. \nprint(\"R Squared (R2) on train data = %g\" % trainingSummary.r2)\nlr_predictions = lr_model.transform(test_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Coefficients: [1.032659502255893,-0.03062986260394673]\nIntercept: -0.1333491411478786\nRoot Mean Squared Error (RMSE) on train data = 1.58299\nR Squared (R2) on train data = 0.99616\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["###QUESTION 3\n##Getting the predicted Apple Stock price for 2016-2017 in our test date and comparing it to the Observed Apple Stock price for 2016-2017 in the test data\nlr_predictions.select(\"features\", \"close\", \"prediction\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+-----------------+\n            features|    close|       prediction|\n+--------------------+---------+-----------------+\n[90.339996,92.510...|90.519997|90.32353751126067|\n[92.040001,93.400...|93.589996|92.05180325067569|\n[92.510002,93.419...|90.339996|92.53654177466282|\n[92.720001,93.239...|92.790001|92.75891261274577|\n[92.790001,92.720...|93.419998|92.84712621456815|\n[93.239998,94.190...|92.720001|93.26679396394712|\n[93.400002,96.099...|92.040001|  93.373520699892|\n[93.419998,99.989...|94.089996|93.27501959376976|\n[93.589996,92.040...|94.400002|93.69407695964604|\n[93.639999,93.739...|    95.18|93.69364235820021|\n[94.089996,93.419...|97.339996|94.16813759227011|\n   [94.830002,97.82]|93.739998| 94.7975403631794|\n[95.099998,95.330...|95.910004|95.15262259477458|\n[95.529999,94.989...|95.940002|95.60708148920892|\n[95.550003,95.910...|96.099998|95.59955915251724|\n   [96.43,95.220001]|97.900002|96.52943111361023|\n   [96.43,97.339996]|94.480003|96.46449595803918|\n[96.449997,100.69...|96.959999|96.38222968112666|\n[96.639999,93.989...|98.120003| 96.7839633993169|\n[96.660004,97.129...|96.790001|  96.708444014713|\n+--------------------+---------+-----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["###QUESTION 4\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"close\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n##Evaluating our model on the test data and storing the results in test_result\ntest_result = lr_model.evaluate(test_df)\n##Using the evalution metric RMSE on our Model to get the RMSE of the test data\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">R Squared (R2) on test data = 0.996224\nRoot Mean Squared Error (RMSE) on test data = 1.61969\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["###BONUS QUESTION\ndf2.describe().show()\ndf2 = df2.withColumn('index', f.monotonically_increasing_id())\nnew_train = df2.sort('index').limit(350)\nnew_test = df2.sort('index', ascending=False).limit(151)\nnew_train.count()\nnew_test.count()\nnew_train.select(f.min('index').alias('min'), f.max('index').alias('max')).show()\nnew_test.select(f.min('index').alias('min'), f.max('index').alias('max')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+\nsummary|             close|\n+-------+------------------+\n  count|               501|\n   mean|127.62568848902191|\n stddev|  25.7860344769813|\n    min|         90.339996|\n    max|        176.419998|\n+-------+------------------+\n\n+---+---+\nmin|max|\n+---+---+\n  0|349|\n+---+---+\n\n+---+---+\nmin|max|\n+---+---+\n350|500|\n+---+---+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["##Initiating the LinearRegression model from the SparkMLLib and specifying the features and the observed value/labels\nlr1 = LinearRegression(featuresCol = 'features', labelCol='close')\n##Fitting the Linear Regression Model on the training data and storing the model in lr_model\nlr_model1 = lr1.fit(new_train)\n##Printing the coefficients of the slope of our Linear Regression Model and the Intercept\nprint(\"Coefficients: \" + str(lr_model1.coefficients))\nprint(\"Intercept: \" + str(lr_model1.intercept))\n##Summarizing our model over the training set and printing out two important metrics - R2 and the RMSE\ntrainingSummary1 = lr_model1.summary\n##RMSE is \nprint(\"Root Mean Squared Error (RMSE) on train data = %g\" % trainingSummary1.rootMeanSquaredError)\n##R2 is \"(total variance explained by model) / total variance.” So if it is 100%, the two variables are perfectly correlated, i.e., with no variance at all. \nprint(\"R Squared (R2) on train data = %g\" % trainingSummary1.r2)\nlr_predictions1 = lr_model1.transform(new_test)\nlr_predictions1.select(\"features\", \"close\", \"prediction\").show()\nlr_evaluator1 = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"close\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator1.evaluate(lr_predictions1))\n##Evaluating our model on the test data and storing the results in test_result\ntest_result1 = lr_model1.evaluate(new_test)\n##Using the evalution metric RMSE on our Model to get the RMSE of the test data\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result1.rootMeanSquaredError)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Coefficients: [1.0497021033467877,-0.04670693092307409]\nIntercept: -0.2014341688800502\nRoot Mean Squared Error (RMSE) on train data = 1.45062\nR Squared (R2) on train data = 0.992977\n+--------------------+----------+------------------+\n            features|     close|        prediction|\n+--------------------+----------+------------------+\n[171.080002,170.6...|169.229996| 171.4134010753746|\n[170.600006,170.5...|171.080002| 170.9109494257973|\n[170.570007,175.0...|170.600006|170.67208119958372|\n[175.009995,175.0...|170.570007|175.33274594201822|\n[175.009995,174.3...|175.009995| 175.3635720026512|\n[174.350006,174.5...|175.009995| 174.6619064514802|\n[174.539993,176.4...|174.350006|174.77352694131872|\n[176.419998,173.9...|174.539993| 176.8614039847619|\n[173.970001,172.2...|176.419998|174.37137410978397|\n[172.220001,172.2...|173.970001|172.53205994226013|\n[172.270004,171.6...|172.220001|172.61117147410846|\n[171.699997,172.6...|172.270004|171.96752815758376|\n[172.669998,169.3...|171.699997| 173.1398732596992|\n[169.369995,169.3...|172.669998|169.67818795561146|\n[169.320007,169.0...|169.369995| 169.6401951559387|\n[169.009995,169.6...|169.320007|169.28534935416667|\n[169.639999,169.8...|169.009995|169.93919258230815|\n[169.800003,171.0...|169.639999|170.04876545399821|\n[171.050003,171.8...|169.800003|171.32352739832245|\n[171.850006,169.4...|171.050003|172.27398812346317|\n+--------------------+----------+------------------+\nonly showing top 20 rows\n\nR Squared (R2) on test data = 0.961641\nRoot Mean Squared Error (RMSE) on test data = 1.88539\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"Assignment 2 section 1","notebookId":4311070677794584},"nbformat":4,"nbformat_minor":0}
